摘自：刘未鹏, http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/

### 贝叶斯公式与全概率公式

**条件概率公式：**
$$
P(A|B) = \frac{P(AB)}{P(B)}
$$
**完备事件组：**

若事件组$\ (A_{1},…,A_{i},...,A_{n})$ 满足条件

1. $\ A_{i}, i=1,…n $ 两两互不相容，且$\ P(A_{i}) > 0$ 
2. $\ \sum_{i=1}^{n}A_{i} = \Omega $

则称事件组$\ (A_{1},…,A_{i},...,A_{n})$ 为$\ \Omega$ 的一个完备事件组，也称$\ \Omega$ 的一个分割

**全概率公式：**

设$\ (A_{1},…,A_{i},...,A_{n})$ 是一个完备事件组，则有
$$
P(B) = \sum_{i=1}^{n}P(A_{i})P(B|A_{i})
$$
**贝叶斯公式（由条件概率和全概率公式导出）：**
$$
P(A_{i}|B) = \frac{P(A_{i})P(B|A_{i})}{\sum_{j=1}^{n}P(A_{j})P(B|A_{j})}
$$

### 历史

所谓的贝叶斯方法源于他生前为解决一个“逆概”问题写的一篇文章，在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如：

>  假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大。

而一个自然而然的问题是反过来：

> 如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测

这个问题，就是所谓的逆概问题。

实际上，贝叶斯当时的论文只是对这个问题的一个直接的求解尝试，并不清楚他当时是不是已经意识到这里面包含着的深刻的思想。

然而后来，贝叶斯方法席卷了概率论，并将应用延伸到各个问题领域，所有需要作出概率预测的地方都可以见到贝叶斯方法的影子，特别地，贝叶斯是机器学习的核心方法之一。

这背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力是有局限性的（否则有很大一部分科学就没有必要做了——设想我们能够直接观察到电子的运行，还需要对原子模型争吵不休吗？），我们日常所观察到的只是事物表面上的结果，沿用刚才那个袋子里面取球的比方，我们往往只能知道从里面取出来的球是什么颜色，而并不能直接看到袋子里面实际的情况。这个时候，我们就需要提供一个猜测（hypothesis，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点），所谓猜测，当然就是不确定的（很可能有好多种乃至无数种猜测都能满足目前的观测），但也绝对不是两眼一抹黑瞎蒙——具体地说，我们需要做两件事情：

1. 算出各种不同猜测的可能性大小。
2. 算出最靠谱的猜测是什么。

第一个就是计算特定猜测的后验概率，对于连续的猜测空间则是计算猜测的概率密度函数。第二个则是所谓的模型比较，模型比较如果不考虑先验概率的话就是最大似然方法。

### 贝叶斯公式

wikipedia 上的一个例子：

> 一所学校里面有 60% 的男生，40% 的女生。
>
> 男生总是穿长裤，女生则一半穿长裤一半穿裙子。
>
> 有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。
>
> 然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近似，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？

一些认知科学的研究表明（《决策与判断》以及《Rationality for Mortals》第12章：小孩也可以解决贝叶斯问题），我们对形式化的贝叶斯问题不擅长，但对于以频率形式呈现的等价问题却很擅长。在这里，我们不妨把问题重新叙述成：

>  你在校园里面随机游走，遇到了 N 个穿长裤的人（仍然假设你无法直接观察到他们的性别），问这 N 个人里面有多少个女生多少个男生。

你说，这还不简单：算出学校里面有多少穿长裤的，然后在这些人里面再算出有多少女生，不就行了？

我们来算一算：假设学校里面人的总数是$\ U$ 个，60% 的男生都穿长裤，于是我们得到了$\  U \times P(Boy) \times P(Pants|Boy) $ 个穿长裤的男生（其中 P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；P(Pants|Boy) 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤）。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了$\ U \times P(Girl) \times P(Pants|Girl)$ 个穿长裤的（女生）。加起来一共是$\ U \times P(Boy) \times P(Pants|Boy) + U \times P(Girl) \times P(Pants|Girl)$ 个穿长裤的，其中有 $\ U \times P(Girl) \times P(Pants|Girl) $个女生。两者一比就是你要求的答案。

下面我们把这个答案形式化一下：我们要求的是$\ P(Girl|Pants) $（穿长裤的人里面有多少女生），我们计算的结果是 
$$
\frac{U \times P(Girl) \times P(Pants|Girl)}{U \times P(Boy) \times P(Pants|Boy) + U \times P(Girl) \times P(Pants|Girl)}
$$
容易发现这里校园内人的总数是无关的，可以消去。于是得到
$$
P(Girl|Pants) = \frac{P(Girl) \times P(Pants|Girl)}{P(Boy) \times P(Pants|Boy) + P(Girl) \times P(Pants|Girl)}
$$
注意，如果把上式收缩起来，分母其实就是 P(Pants) ，分子其实就是 P(Pants, Girl) 。而这个比例很自然地就读作：在穿长裤的人（ P(Pants) ）里面有多少（穿长裤）的女孩（ P(Pants, Girl) ）。

上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：

A: Pants

B: Gril

~B: Boy
$$
P(B|A) = \frac{P(A|B) \times P(B)}{P(A|B) \times P(B) + P(A|\sim B) \times P(\sim B) }
$$
收缩起来就是：
$$
P(B|A) = \frac{P(AB)}  {P(A)}
$$
其实这个就等于：
$$
P(B|A) \times P(A) = P(AB)
$$
难怪拉普拉斯说**概率论只是把常识用数学公式表达了出来**。

然而，后面我们会逐渐发现，看似这么平凡的贝叶斯公式，背后却隐含着非常深刻的原理。

### 拼写纠正

问题：

我们看到用户输入了一个不在字典中的单词，我们需要去猜测：“这个家伙到底真正想输入的单词是什么呢？”用刚才我们形式化的语言来叙述就是，我们需要求：
$$
P(我们猜测他想输入的单词 | 他实际输入的单词)
$$
这个概率。并找出那个使得这个概率最大的猜测单词。显然，我们的猜测未必是唯一的，就像前面举的那个自然语言的歧义性的例子一样；这里，比如用户输入： thew ，那么他到底是想输入 the ，还是想输入 thaw ？到底哪个猜测可能性更大呢？幸运的是我们可以用贝叶斯公式来直接出它们各自的概率，我们不妨将我们的多个猜测记为 $\ h1，h2 .. $（ h 代表 hypothesis），它们都属于一个有限且离散的猜测空间 H （单词总共就那么多而已），将用户实际输入的单词记为 D （ D 代表 Data ，即观测数据），于是
$$
P(我们的猜测1 | 他实际输入的单词)
$$
可以抽象地记为：
$$
P(h_{1} | D)
$$
类似地，对于我们的猜测2，则是$\  P(h2 | D)$。不妨统一记为：
$$
P(h | D)
$$
运用一次贝叶斯公式，我们得到：
$$
P(h | D) = \frac{P(h) \times P(D | h)}{P(D)}
$$
对于不同的具体猜测 h1 h2 h3 .. ，P(D) 都是一样的，所以在比较 P(h1 | D) 和 P(h2 | D) 的时候我们可以忽略这个常数。即我们只需要知道：
$$
P(h | D) ∝ P(h) \times P(D | h)
$$
 （注：那个符号的意思是“正比例于”，不是无穷大，注意符号右端是有一个小缺口的。）

这个式子的抽象含义是：

> 对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身独立的可能性大小（先验概率，Prior ）”和“这个猜测生成我们观测到的数据的可能性大小”（似然，Likelihood ）的乘积。

具体到我们的那个 thew 例子上，含义就是，用户实际是想输入 the 的可能性大小取决于 the 本身在词汇表中被使用的可能性（频繁程度）大小（先验概率）和 想打 the 却打成 thew 的可能性大小（似然）的乘积。

下面的事情就很简单了，对于我们猜测为可能的每个单词计算一下 P(h) * P(D | h) 这个值，然后取最大的，得到的就是最靠谱的猜测。

**一点注记**：Norvig 的拼写纠正器里面只提取了编辑距离为 2 以内的所有已知单词。这是为了避免去遍历字典中每个单词计算它们的 P(h) * P(D | h) ，但这种做法为了节省时间带来了一些误差。但话说回来难道我们人类真的回去遍历每个可能的单词来计算他们的后验概率吗？不可能。实际上，根据认知神经科学的观点，我们首先根据错误的单词做一个 bottom-up 的关联提取，提取出有可能是实际单词的那些候选单词，这个提取过程就是所谓的基于内容的提取，可以根据错误单词的一些模式片段提取出有限的一组候选，非常快地缩小的搜索空间（比如我输入 explaination ，单词里面就有充分的信息使得我们的大脑在常数时间内把可能性 narrow down 到 explanation 这个单词上，至于具体是根据哪些线索——如音节——来提取，又是如何在生物神经网络中实现这个提取机制的，目前还是一个没有弄清的领域）。然后，我们对这有限的几个猜测做一个 top-down 的预测，看看到底哪个对于观测数据（即错误单词）的预测效力最好，而如何衡量预测效率则就是用贝叶斯公式里面的那个 P(h) * P(D | h) 了——虽然我们很可能使用了[一些启发法来简化计算](http://www.douban.com/subject/1599035/)。后面我们还会提到这样的 bottom-up 的关联提取。