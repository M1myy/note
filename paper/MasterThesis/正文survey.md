众所周知，在分布式平台上安排多任务工作流是一个NP硬问题[17]。因此，制定最佳的调度计划非常耗时，通常具有指数时间复杂性。幸运的是，具有多项式复杂度的启发式和元启发式算法能够以可接受的最优性损失为代价，为网格，群集和云计算生成调度计划的近似或近似最优解。

例如，Mao等[28]开发Scaling-Consolidation-Scheduling算法来调度云上的工作流程。他们的算法旨在找到合并异构虚拟机的最优调度计划。他们考虑到性能变化不变（20％）。 MEENA等人[这个？]考虑了类似的有界性能变化，并使用遗传算法来生成调度计划。 Malawski等人[29]引入了三种算法，DPDS（动态配置动态调度），WA-DPDS（Workflow-Aware DPDS）和SPSS（静态配置静态调度），以在云中运行多个工作流。他们提出的算法旨在最大化在给定的期限和成本的限制下执行的工作流数量。但是，在VM上执行时，他们认为工作流任务具有不断的执行时间。

Abrishami等[30]，Calheiros等[31]，Poola等[32]和Sahni等[33]提出启发式算法，用于在IaaS云上使用单个工作流实例进行调度。 Abrishami等人[30]引入静态IaaS云部分关键路径（IC-PCP）算法来评估任务的最终完成时间，然后识别每个部分关键路径（PCP），并最终将部分关键路径上的每个任务分配给最便宜的VM实例。如果算法无法在完成时间约束之后找到任何可用的VM实例，则会生成一个新的最便宜的VM实例，在其最后的完成时间之前执行所有任务。 Calheiros等人[31]在复制任务时考虑工作流程的软期限（即SLA违规率）。他们提出了一种利用提供资源的空闲时间复制工作流任务的Enhance-IaaS-Cloud-Partial-Critical-Path（EIPR）算法。他们考虑到虚拟机的执行时间方面的界限（高达10％）的性能变化。 Poola等人[32]提出一个类似的框架，并考虑在找到最优调度计划时的容错能力。 Sahni等人[33]提出了一种动态的成本效益工作流调度算法。该算法首先决定用户定义的最后期限是否可实现。如果没有，则在任务准备运行之前产生一个次优解决方案。同样，他们也考虑到VM的有限变体。

Byun等人[36]为网格环境引入了平衡时间调度（BST）算法，以估计最终时间约束下工作所需的计算资源的最小化规模。 BTS算法在不违反其时间约束的条件下尽可能地延迟任务。然而，为了简单起见，他们考虑使用均匀的VM后来，Byun等人[34]提出了一种基于云的工作流调度的改进的分区平衡时间调度（PBTS）算法。该算法评估在给定的期限约束之后执行工作流所需的资源的最小容量。该算法仍然假定均匀的VM。
吴等[35]提出了一个执行时间最小化算法，用于限制时间限制的工作流程。它们定义了保证结束期限所需VM数量的下限和上限。然后，他们开发了一种启发式方法，将任务调度到分配的VM实例，并采用IHM（实例小时最小化）算法来减少虚拟机所花费的时间。

另一类解决方案是基于元启发式算法。这些工作考虑了云中的单一工作流程环境。例如，Pandey等人[37]旨在通过平衡可用资源上的负载来最小化工作流的执行成本。他们考虑资源池中的一组固定的VM来分配任务。罗德里格斯等人[38]提出了粒子群优化（PSO）调度算法。他们的方法基于代表粒子位置的资源索引编码粒子。然而，规定颗粒在不同的维度上移动，因此解决方案的全局最优性不能得到保证。 Chen et al。 [39]采用类似的编码方案[38]，并基于动态客观策略介绍了成本优化的限期约束调度。他们提出了一种动态客观策略，当没有可行的解决方案时，从执行成本函数切换到执行时间。 Zhu et al。 [40]提出了类似的优化公式，但他们认为云中的虚拟机具有不变的和恒定的性能。

**总结缺陷**

可以看出，上述工作的主要缺陷是它们都假设虚拟机的性能是恒定不变的或者是在有界的范围内波动的，而这一假设对科学工作流调度造成的限制是多重的，主要体现在：

1. 对于现实应用中的云，特别是用于科学计算应用的异构和分布式云数据中心，性能和质量的波动是存在的并且是普遍的。这种波动是由数据中心之间/云节点之间的网络连接恶化和恢复或者物理机硬件动态调节、缩放造成的。假设虚拟机具有恒定的性能（通常由计算平均历史性的得到），将其作为算法输入并得到调度计划，这样解决的问题就是一个静态的调度问题，没有考虑到系统性能动态变化，这种情况下当云端资源压力较大时，就容易导致高 SLA 违背率和带来不良的用户体验。
2. 假设虚拟机具有有界性能并将其作为算法输入，这样可以避免在云性能不佳时 SLA 违背率过高。然而，这种假设可能通过对系统性能进行悲观估计从而浪费资源。假设在一个廉价的虚拟机 $vm_1$ 上执行任务 $t_1$ 的平均/最高执行时间为10s/13s，而另外一个昂贵的虚拟机 $vm_2$ 平均/最高执行时间为7s / 8s。如果用户不能容忍执行时间超过 12 秒，则采用有界性能假设的调度算法可能会选择昂贵的虚拟机来避免违背 SLA，然而，执行任务 $t_1$ 超过 13s 这种情况通常发生在端资源压力较大时。因此，更加智能的算法应该可以感知到云上资源性能变化的趋势（上行或者下行），预测出虚拟机的未来性能，并选择合适的虚拟机进行任务分配。
3. 有的学者指出（例如[123-123]）认为虚拟机执行任务所花费的时间服从指数分布，虽然这样的假设可以通过使用马尔可夫表现模型将问题变得容易解决，但是在实际的应用环境下，马尔可夫模型的无记忆性显然是不切实际的，实验数据表明，完成时间不仅依赖于当前的状态，而且还依赖于历史数据。
4. 最近有些学者[general-model？]假设完成时间服从一般分布而不是指数分布。他们认为任务完成时间的历史经验分布是理论分布。一个主要的局限性在于，历史经验分布只是使用样本密度作为其分布概率，而忽略了其变化的趋势。xia在[16]中讨论了这种情况：任务执行时间的分配随着时间的延长随着时间的延长而出现明显提高了性能的恶化。然而，其经验分布可以与具有稳定出现的长时间延迟随时间的另一个响应延迟类型的经验分布定量相同。通过使用基于时间序列的分析可以很好地避免上述限制。因此，我们引入动态预测方法（使用自回归 - 移动平均模型（ARMA）[？]系列模型），特别注意VM的性能运行时间趋势。然后，我们为遗传算法提供预测各个虚拟机的输入性能，并在运行时生成调度计划。