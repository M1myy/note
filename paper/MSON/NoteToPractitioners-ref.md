Analyzing their QoS plays an important role in understanding and improving the quality of cloud computing systems and cloud-based applications. However, it is a greatchallenge with their increasing scale and complexity. 

A careful investigation into existing methods has found their limitations when applied to address the QoS analysis challenge: 1) incomplete modeling of details such as machine provisioning, request handling, and machine failure/repair process; 2) reliance on measurement-based QoS data that require extensive experimentation and real-system tests; and 3) separate modeling of provisioning phases which simplifies the solution but loses accuracy. 

To overcome these limitations, we propose a comprehensive quality determination framework for Infrastructure-as-a-Service clouds. It features with: 1) taking machine provisioning, request handling, machine occupation/releasing, and machine failure/repair process mechanism into a unified analytical model; 2) employing queueing networks as the fundamental means of QoS analysis; 3) analytical solutions of three important QoS metrics; 4) analytical modeling of different strategies of machine warm-up/cool-down and the ability to identify the optimal balance between system overhead and performance; and 5) a confidence interval analysis to validate model correctness based on simulative QoS data. 

The proposed framework can aid the design and optimization of industrial cloud computing systems and practitioners’ understanding of QoS aspects of cloud-based applications.



​		
Cloud computing is recognized to be highly suitable for supporting large-scale scientific applications
expressed as scientific workflows. However, it remains a challenge to schedule workflow tasks and allocate cloud resources with least financial cost and acceptable quality assurance. 

A careful investigation into existing methods has found their various limitations, especially the assumption of time-invariant, stochastic (with assumed or derived distributions), or bounded performance of VMs. 

In this work, we introduce a novel method for optimal scientific workflow scheduling on IaaS clouds. Instead of considering constant, stochastic, or bounded performance of VMs, our proposed method models time-varying performance based on a time-series model and employs genetic algorithms to yields cost-effective schedules to reduce workflow cost while meeting the constraints of Service-Level-Agreement (SLA). 

A case study based on real-world commercial IaaS clouds and some well-known scientific workflows suggests that our proposed method outperforms traditional approaches that consider time-invariant or bounded VM performance only. The proposed method can aid the design and optimization of industrial cloud computing systems for large-scale scientific applications and help practitioners understand quality aspects of scientific workflows.



The increasing complexity of the real-world industry processes inevitably leads to the occurrence of nonlinearity and high dimensions, and their mathematical models are often difficult to build. How
to design the optimal controller for nonlinear systems without the requirement of knowing the explicit model has become one of the main foci of control practitioners. 

However, this problem cannot be handled by only relying on the traditional dynamic programming technique because of the "curse of dimensionality". To make things worse, the backward direction of solving process of dynamic programming precludes its wide application in practice. Therefore, in this paper, the iterative adaptive dynamic programming algorithm is proposed to deal with the optimal control problem for a class of unknown nonlinear systems forward-in-time. Moreover, the detailed implementation of the iterative ADP algorithm through the globalized dual heuristic programming technique is also presented by using neural networks. 

Finally, the effectiveness of the control strategy is illustrated via simulation study.


​			
​		
​	


​			
​		
​				
​		
​	